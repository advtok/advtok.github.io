<!DOCTYPE html>
<html>
<head>

  <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1KVB45JMTX"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-1KVB45JMTX');
    </script>

  <meta charset="utf-8">
  <meta name="description"
        content="Adversarial Tokenization">
  <meta name="keywords" content="Adversarial attacks">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Adversarial Tokenization</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="http://starai.cs.ucla.edu">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="http://starai.cs.ucla.edu">
            StarAI Lab
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Adversarial Tokenization</h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://web.cs.ucla.edu/~renatolg/">Renato Lui Geh</a><sup>*</sup>,</span>
            <span class="author-block"><a href="https://zoeshao0425.github.io/">Zilei Shao</a><sup>*</sup>,</span>
            <span class="author-block"><a href="https://web.cs.ucla.edu/~guyvdb/">Guy Van den Broeck</a></span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">University of California, Los Angeles</span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
          </div>
        </div>
      </div>
      
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="publication-links">
            <!-- PDF Link. -->
            <span class="link-block">
              <a href="https://arxiv.org/pdf/2503.02174" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://arxiv.org/abs/2503.02174" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
              </a>
            </span>
            <!-- Code Link. -->
            <span class="link-block">
              <a href="https://github.com/RenatoGeh/advtok/" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://pypi.org/project/advtok/" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-download"></i></span>
                <span>Package</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
          <!-- TL;DR Section -->
        <div class="content is-size-5 has-text-centered">
          <em><strong>TL;DR:</strong> We show a previously unknown vulnerability of LLMs in addressing tokenization attacks whereby simply retokenizing an unsafe request elicits dangerous responses in state-of-the-art LLMs.</em>
        </div>
        <h3 class="title is-4">Canonical Tokenization (üö´ Blocked)</h3>
        <div class="publication-image">
          <img src="./static/images/safe.gif" alt="Canonical Tokenization" class="responsive-image" loop autoplay style="max-width: 650px;">
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Adversarial Tokenization (üîì Bypassed)</h3>
        <div class="publication-image">
          <img src="./static/images/unsafe.gif" alt="Adversarial Tokenization" class="responsive-image" loop autoplay style="max-width: 650px;">
        </div>
      </div>
    </div>

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Current LLM pipelines account for only one possible tokenization for a given string, ignoring exponentially many alternative tokenizations during training and inference.
            For example, the standard <code>Llama3</code> tokenization of "penguin" is "[p,enguin]", yet "[peng,uin]" is another perfectly valid alternative.
            In this paper, we show that despite LLMs being trained solely on one tokenization, they still retain semantic understanding of other tokenizations, raising questions about their implications in LLM safety.
            Put succinctly, we answer the following question: <em>can we adversarially tokenize an obviously malicious string to evade safety and alignment restrictions?</em>
          </p>
          <p>
            We show that not only is adversarial tokenization an effective yet previously neglected axis of attack, but it is also competitive against existing state-of-the-art adversarial approaches without changing the text of the harmful request.
          </p>
          <p>
            We empirically validate this exploit across three state-of-the-art LLMs and adversarial datasets, revealing a previously unknown vulnerability in subword models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Introduction -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Tokenization is a critical but often overlooked component of Large Language Model (LLM) pipelines. A <strong>canonical tokenization</strong> is the standard way a tokenizer segments a given string, while <strong>noncanonical tokenizations</strong> are alternative segmentations that the tokenizer could have chosen but didn't. 
            For example, the word "penguin" is canonically tokenized as <strong>[p, enguin]</strong> by Llama3, but <strong>[peng, uin]</strong> is another valid (noncanonical) tokenization. 
          </p>
          <p>
            Our work focuses on how these alternative, or ‚Äúnoncanonical,‚Äù tokenizations preserve meaning, and can be manipulated to bypass safety filters without altering the original text. By forcing models to act on tokenizations they never saw during training, we can evade alignment safeguards and prompt harmful responses. 
          </p>
          <p>
            This phenomenon exposes not only the vulnerability of LLMs against <strong>adversarial tokenization</strong>, but also fundamental issues with the current LLM safety training pipeline.
          </p>
        </div>
      </div>
    </div>
    <!--/ Introduction -->

    <!-- Semantic Signal Retention -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Semantic Signal Retention</h2>
        <div class="content has-text-justified">
          <p>
            Despite only being trained on the canonical segmentation, we find that LLMs maintain an impressive grasp of noncanonical tokenizations. 
          </p>
          <p>
            We measure this by quizzing the models with multiple-choice questions (example shown below) in which each question is tokenized more and more deviated from the canonical one.
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <div class="publication-image">
                  <img src="./static/images/question.jpg" alt="Adversarial Tokenization" class="responsive-image" loop autoplay style="max-width: 600px;">
                </div>
              </div>
            </div>
          </p>
          <p>
            As expected, accuracy tends to drop when the tokenization deviates further from the canonical. More importantly, the trend is smooth in the sense that noncanonical tokenizations close to the canonical are not too noisy.
          </p>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="publication-image">
              <img src="./static/images/semantic_signal.jpg" alt="Adversarial Tokenization" class="responsive-image" loop autoplay style="max-width: 600px;">
              <p class="caption">In the figure, the x-axis represents the normalized edit distance from the canonical tokenization (ranging from 0 to 1), capturing how far each alternative tokenization diverges in terms of insertions or substitution.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Semantic Signal Retention -->

    <!-- Evades Safety -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Tokenizations Can Evade Safety</h2>
        <div class="content has-text-justified">
          <p>
            To test the hypothesis that noncanonical tokenizations to possibly evade alignment by accessing the distribution conditioned on them, we sample tokenizations of a malicious question from different distances and evaluate whether the LLM faithfully answers the malicious request.
          </p>
          <p>
            We evaluate responses with StrongREJECT (ranging from 0 to 1; higher scores indicate more accurate nonrefusal responses that are relevant to the question).
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <div class="publication-image">
                  <img src="./static/images/compliance.png" alt="Adversarial Tokenization" class="responsive-image" loop autoplay style="max-width: 600px;">
                </div>
              </div>
            </div>
          </p>
          <p>
            Unsurprisingly, the canonical tokenization tends to have the lowest scores. Notably though, distance from canonicity seems to play a role in how well it performs against complying to a malicious request, meaning that by simply sampling tokenizations at a sufficiently large distance from the canonical we can successfully provoke unsafe responses from LLMs.
          </p>
        </div>
      </div>
    </div>
    <!--/ Evades safety -->

    <!-- Adversarial Tokenization Algorithm -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Adversarial Tokenization</h2>
        <div class="content has-text-justified">
          <p>
            To reliably find such tokenizations that evade from safety constraints, we devise a simple yet effective local search algorithm for adversarially finding tokenizations that elicit a desired behavior from the LLM.
          </p>
          <p>
            Because the problem was proven to be NP-hard, this greedy approach picks small local changes at each iteration to produce a tokenization maximizing the probability of generating a given malicious response. We refer interested readers to the paper for full details and proofs.
          </p>
        </div>
      </div>
    </div>
    <!-- /Adversarial Tokenization Algorithm -->

    <!-- Case Studies -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Case Studies</h2>
        <div class="content has-text-justified">

          <!-- Jailbreaking -->
          <h3 class="title is-5">Jailbreaking</h3>
          <p>
            In the jailbreaking task, given a malicious request, the goal is to construct an attack input prompt that successfully provokes the LLM to output a response that faithfully answers the prompt.
          </p>
          <p>
            We compare our method against three other jailbreak methods: GCG, AutoDAN, and FFA.
          </p>
          <p>
            Because adversarial tokenization does not change the underlying text of the request, we can further boost these three previous methods with ours by simply reusing the same adversarial tokenizations used on the malicious requests and plugging them into their attack templates or affixes.
          </p>

          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-image">
                <img src="./static/images/jailbreak.png" alt="Adversarial Tokenization" class="responsive-image" loop autoplay style="max-width: 600px;">
              </div>
            </div>
          </div>

          Notably, our approach seems to perform especially well on Llama3, achieving best scores as a standalone attack and boosting the performance of other methods when combined. In the case of both Gemma2 and OLMo2, adversarial tokenization by itself achieved competitive results against others, but especially shined when combined with AutoDAN.

          <!-- Safety Model Evasion -->
          <h3 class="title is-5">Evading Safety Models</h3>
          <p>
            We also show that adversarial tokenization increases the probability of bypassing the existing defense layers against malicious requests that reliably distinguish whether a prompt or response is (un)safe.
          </p>
          <p>
            We evaluate both LlamaGuard and ShieldGemma, which allow for computing the probability of a prompt being unsafe, and our results show that adversarial tokenization substantially increases the probability of bypassing these safety checks.
          </p>

          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-image">
                <img src="./static/images/safety_evasion.png" alt="Adversarial Tokenization" class="responsive-image" loop autoplay style="max-width: 600px;">
              </div>
            </div>
          </div>

          <!-- Prompt Injection -->
          <h3 class="title is-5">Prompt Injection</h3>
          <p>
            This man-in-the-middle attack consists of a setting where a user inputs a harmless query to an LLM and a malicious agent intercepts it, possibly altering the user input to provoke a malicious response. Here, we specifically consider payloads that request the LLM to be toxic and offensive. We define success as an attack where the adversarial content is a substring of the generated response and does not produce refusal text, as shown below.
          </p>
          
          <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <div class="publication-image">
                  <img src="./static/images/prompt_injection_example.png" alt="Adversarial Tokenization" class="responsive-image" loop autoplay style="max-width: 600px;">
                </div>
              </div>
            </div>
          <p>
            <p>
              The following figure shows success rates for both canonical tokenization baseline and adversarial tokenization, revealing a consistent increase in success when using adversarial tokenization.
            </p>
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <div class="publication-image">
                  <img src="./static/images/prompt_injection.png" alt="Adversarial Tokenization" class="responsive-image" loop autoplay style="max-width: 600px;">
                </div>
              </div>
            </div>
        </div>
      </div>
    </div>
    <!-- /Case Studies -->

    <!-- Discussion -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Discussion</h2>
        <div class="content has-text-justified">
          <p>
            In this paper, we have shown how noncanonical tokenizations expose a serious vulnerability in LLM alignment for safety. Adversarial tokenization is able to access the out-of-distribution regions of alignment but remain close enough to the data distribution of the pre-trained LLM, allowing them to evade alignment and elicit unsafe behavior from models.
          </p>
          <p>
            We argue that the problem lies deeper: the current LLM safety training pipeline is flawed. While in pre-training the semantics of a text ends up being leaked onto many tokenizations, allowing for meaningful responses from noncanonical tokenizations (as shown in Figure 4), the smaller scale of post-training might not allow for that, leading to adversarial tokenizations.
          </p>
          <p>
            Our work exposes not only the vulnerability of LLMs against adversarial tokenization, but also fundamental issues with the current LLM safety training pipeline.
          </p>
        </div>
      </div>
    </div>
    <!--/ Discussion -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{geh2025adversarialtokenization,
      title={Adversarial Tokenization}, 
      author={Renato Lui Geh and Zilei Shao and Guy Van den Broeck},
      year={2025},
      eprint={2503.02174},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.02174}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a
            href="https://nerfies.github.io/">Nerfies project page</a>.
          </p>
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
